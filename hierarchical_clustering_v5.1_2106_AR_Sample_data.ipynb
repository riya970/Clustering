{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0910b3",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78b1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e2db0",
   "metadata": {},
   "source": [
    "# Function Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5a09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tukey's method defination\n",
    "def tukeys_method(df, variable):\n",
    "    #Takes two parameters: dataframe & variable of interest as string\n",
    "    q1 = df[variable].quantile(0.25)\n",
    "    q3 = df[variable].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    inner_fence = 1.5*iqr\n",
    "    outer_fence = 3*iqr\n",
    "    \n",
    "    #inner fence lower and upper end\n",
    "    inner_fence_le = q1-inner_fence\n",
    "    inner_fence_ue = q3+inner_fence\n",
    "    \n",
    "    #outer fence lower and upper end\n",
    "    outer_fence_le = q1-outer_fence\n",
    "    outer_fence_ue = q3+outer_fence\n",
    "    \n",
    "    outliers_prob = []\n",
    "    outliers_poss = []\n",
    "    for index, x in enumerate(df[variable]):\n",
    "        if x <= outer_fence_le or x >= outer_fence_ue:\n",
    "            outliers_prob.append(index)\n",
    "    for index, x in enumerate(df[variable]):\n",
    "        if x <= inner_fence_le or x >= inner_fence_ue:\n",
    "            outliers_poss.append(index)\n",
    "    return outliers_prob, outliers_poss\n",
    "\n",
    "# EDD function\n",
    "def descriptive_statistics_summary(df):\n",
    "    df_desc_statistics = df.describe().transpose()\n",
    "    variable_type = df.dtypes\n",
    "    variable_count = df.count()\n",
    "    miss_val_count = df.isnull().sum()\n",
    "    miss_val_percent = 100 * df.isnull().sum()/len(df)\n",
    "    unique_count = df.nunique()\n",
    "    \n",
    "    median = df.median().astype('object')\n",
    "    quan_0_1 = df.quantile(0.01).transpose()\n",
    "    quan_1 = df.quantile(0.1).transpose()\n",
    "    quan_99 = df.quantile(.99).transpose()\n",
    "    \n",
    "    columns = df.keys()\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        col_dtype = df[col].dtypes\n",
    "        \n",
    "        if col_dtype == 'object':\n",
    "            unique_counts = df[col].value_counts(dropna = 0)\n",
    "            median.at[col] = str(unique_counts.index[0]) + ':' + str(unique_counts[0])\n",
    "            quan_0_1.at[col] = str(unique_counts.index[1]) + ':' + str(unique_counts[1]) if len(unique_counts) >= 2 else \"0\"\n",
    "            quan_1.at[col] = str(unique_counts.index[2]) + ':' + str(unique_counts[2]) if len(unique_counts) >= 3 else \"0\"\n",
    "            quan_99.at[col] = str(unique_counts.index[3]) + ':' + str(unique_counts[3]) if len(unique_counts) >= 4 else \"0\"\n",
    "            df_desc_statistics.at[col, 'mean'] = str(unique_counts.index[4]) + ':' + str(unique_counts[4]) if len(unique_counts) >= 5 else \"0\"\n",
    "\n",
    "        \n",
    "    miss_val_table = pd.concat(\n",
    "            [\n",
    "                variable_type,\n",
    "                variable_count,\n",
    "                miss_val_count,\n",
    "                miss_val_percent,\n",
    "                unique_count,\n",
    "                median,\n",
    "                quan_0_1,\n",
    "                quan_1,\n",
    "                quan_99,\n",
    "                df_desc_statistics['mean'],\n",
    "                df_desc_statistics['std'],\n",
    "                df_desc_statistics['min'],\n",
    "                df_desc_statistics['25%'],\n",
    "                df_desc_statistics['50%'],\n",
    "                df_desc_statistics['75%'],\n",
    "                df_desc_statistics['max']\n",
    "            ], axis=1).rename(\n",
    "    columns = {\n",
    "                0 : 'Variable Type',\n",
    "                1 : 'Variable Count',\n",
    "                2 : 'Missing Value Count',\n",
    "                3 : '% Total Missing Values',\n",
    "                4 : 'Unique Count',\n",
    "                5 : 'Median',\n",
    "                6 : '1%'\n",
    "    })\n",
    "    \n",
    "    return miss_val_table\n",
    "\n",
    "# pre-post export\n",
    "def pp_export(pre_data,post_data,filename):\n",
    "    with pd.ExcelWriter(f'{output_path}/{filename}.xlsx') as writer:\n",
    "        pre_data.to_excel(writer, sheet_name='Pre_treatment',index=False)\n",
    "        post_data.to_excel(writer, sheet_name='Post_treatment',index=False)\n",
    "        descriptive_statistics_summary(pre_data).to_excel(writer, sheet_name='EDD_Pre_treatment',index=True)\n",
    "        descriptive_statistics_summary(post_data).to_excel(writer, sheet_name='EDD_Post_treatment',index=True)\n",
    "\n",
    "        \n",
    "def get_binary_col(df,value_dict):\n",
    "    for col in value_dict:\n",
    "        val_1 = list(value_dict[f'{col}'][0].keys())[0]\n",
    "        val_2 = list(value_dict[f'{col}'][0].keys())[1]\n",
    "        \n",
    "        val_1_list = value_dict[f'{col}'][0][val_1]\n",
    "        val_2_list = value_dict[f'{col}'][0][val_2]\n",
    "        \n",
    "        df[f'{col}'] = df[f'{col}'].apply(lambda x : val_1 if x in val_1_list else (val_2 if x in val_2_list else None))\n",
    "    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b3b66",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d43279",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Input_data/Masked_Mercury Financial Sample.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Input_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Output_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m data_orig \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/Masked_Mercury Financial Sample.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Input_data/Masked_Mercury Financial Sample.csv'"
     ]
    }
   ],
   "source": [
    "input_path = './Input_data'\n",
    "output_path = './Output_data'\n",
    "data_orig = pd.read_csv(f\"{input_path}/Masked_Mercury Financial Sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8bd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_orig.copy()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05797f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182341d",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "### Data merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b365ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal + Bureau datasets\n",
    "# data = pd.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc2b5f",
   "metadata": {},
   "source": [
    "### Deleting insignificant records and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unrequired columns\n",
    "redundant_cols = ['Sr. no'\n",
    "                ,'CE_Individual_HoH_Title_Code'\n",
    "                ,'CE_Buyer_Behavior_Cluster_Code_Filler'\n",
    "                ,'CE_Delivery_Point'\n",
    "                ,'CE_House_Fraction'\n",
    "                ,'CE_Match_Level'\n",
    "                ,'CE_Match_Score'\n",
    "                ,'CE_Route_Number'\n",
    "                 ]\n",
    "data.drop(columns=redundant_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for null values across columns\n",
    "record_na_thresh = int(data.shape[1] * 0.5)\n",
    "drop_record_index = list(data[data.isna().sum(axis=1) > record_na_thresh].index)\n",
    "data.drop(index=drop_record_index,axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7938a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'1.Insig_record_drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c21389",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "### *Insights* : \n",
    "- Data contains integer and categorical value columns\n",
    "- There are no missing values\n",
    "- Columns 'Experience' and 'Current_Job_Years' are corelated\n",
    "- No sign for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3c500",
   "metadata": {},
   "source": [
    "### Heatmap/Corelation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.heatmap(data.corr(),cmap=\"YlGnBu\", annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb448c",
   "metadata": {},
   "source": [
    "### Observations distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb9baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = list(data.select_dtypes(exclude=['object']).columns)\n",
    "for col in data.columns:\n",
    "    if col in numeric_cols:\n",
    "        sns.displot(data=data, x=col, kind='kde', fill=True, palette=sns.color_palette('bright'), height=5, aspect=2.5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f7cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if col in numeric_cols:\n",
    "        sns.boxplot(data=data, y=col, palette=sns.color_palette('muted'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fa0ec",
   "metadata": {},
   "source": [
    "### Data transformation, encoding, formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98694f04",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "- Binary encoding on 'Married/Single' and 'Car_Ownership' columns\n",
    "- One hot encoding on House ownership column\n",
    "- Frequency encoding on 'Profession' and 'STATE' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_matching_dict = {'EDUCATION': [{\n",
    "                               'Bachelor': ['BTECH'],\n",
    "                               'Master': ['MTECH','MBA']\n",
    "                                    }]\n",
    "                      }\n",
    "\n",
    "data = get_binary_col(data,value_matching_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c27798",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding\n",
    "col_uniq_count = dict(data.nunique())\n",
    "\n",
    "binary_enc_cols = [key for key in col_uniq_count if col_uniq_count[key] == 2 and key not in numeric_cols]\n",
    "# binary_enc_cols = ['Married/Single','Car_Ownership','MISS_CAT','EDUCATION']\n",
    "\n",
    "\n",
    "for col in binary_enc_cols:\n",
    "    uni_col_vals = list(data[f'{col}'].unique())\n",
    "    uni_col_vals = [val for val in uni_col_vals if str(val).lower() not in ['','nan','na','null']]\n",
    "    data[f'{col}'] = data[f'{col}'].apply(lambda x: 0 if x == uni_col_vals[0] else (1 if x == uni_col_vals[1] else None))\n",
    "#     data = data.drop(columns=[col])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_enc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'2.Binary_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae43916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "# selecting columns for encoding\n",
    "oneHot_enc_cols = [key for key in col_uniq_count if col_uniq_count[key] in ([3,4,5]) and key not in numeric_cols]\n",
    "# oneHot_enc_cols = ['House_Ownership']\n",
    "\n",
    "for col in oneHot_enc_cols:\n",
    "    # creating dummy varibles df\n",
    "    dummy_df = pd.get_dummies(data[f'{col}'], prefix=f'{col}')\n",
    "    \n",
    "    # dummy variables merge into data\n",
    "    dummy_df.drop(columns=dummy_df.columns[0], inplace=True)\n",
    "    data = pd.concat([data,dummy_df],axis='columns')\n",
    "    \n",
    "    # dropping base column\n",
    "    data = data.drop(columns=[col])\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_enc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58182d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'3.One_hot_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding\n",
    "\n",
    "# selecting columns for encoding\n",
    "freq_enc_cols = [key for key in col_uniq_count if col_uniq_count[key] >5 and key not in numeric_cols]\n",
    "# freq_enc_cols = ['Profession','CITY','STATE']\n",
    "\n",
    "for col in freq_enc_cols:\n",
    "    # calculating frequency\n",
    "    col_freq = data.groupby(f'{col}').size()/len(data)\n",
    "    \n",
    "    # mapping values\n",
    "    data.loc[:, f\"{col}_enc\"] = round(data[f'{col}'].map(col_freq),5)\n",
    "    \n",
    "    # dropping base column`\n",
    "    data = data.drop(columns=[col])\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_enc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ca614",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'4.Freq_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31bc0f9",
   "metadata": {},
   "source": [
    "### Dropping corelated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5136916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap over encoded data\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.heatmap(data.corr(),cmap=\"YlGnBu\", annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the corelation threshold\n",
    "corr_thresh = 0.60\n",
    "\n",
    "cor_matrix = data.corr().abs()\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "to_drop_cols = [column for column in upper_tri.columns if any(upper_tri[column] > corr_thresh)]\n",
    "\n",
    "data.drop(columns=to_drop_cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check after dropping corelated columns\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.heatmap(data.corr(),cmap=\"YlGnBu\", annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'5.Corr_var_drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688b1c6",
   "metadata": {},
   "source": [
    "### Dropping insignificant vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e980dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping insignificant varibales\n",
    "\n",
    "# dropping null values columns based on missing values threshold\n",
    "missing_threshold = 0.4\n",
    "\n",
    "non_misisng_count = int((1-missing_threshold) * data.shape[0] + 1)\n",
    "data.dropna(thresh=non_misisng_count, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'6.Insig_var_drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23daa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc6eeb",
   "metadata": {},
   "source": [
    "### Missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling binary columns with same ratio\n",
    "for col in [x for x in binary_enc_cols if x in data.columns]:\n",
    "    total_null = data[col].isna().sum()\n",
    "    total_filled = (data.shape[0]) - total_null\n",
    "    \n",
    "    filled_ratio_val1 = round((data[col].value_counts()[0])/total_filled,1)\n",
    "    filled_ratio_val2 = 1-filled_ratio_val1\n",
    "    \n",
    "    fillna_val1_count = int(filled_ratio_val1 * total_null)\n",
    "    fillna_val2_count = int(total_null-fillna_val1_count)\n",
    "    \n",
    "    null_index = list(data[col].index[data[col].apply(np.isnan)])\n",
    "    \n",
    "    val1_index = random.sample(null_index, fillna_val1_count)\n",
    "    val2_index = [val for val in null_index if val not in val1_index]\n",
    "    \n",
    "    data.loc[val1_index,col] = data[col].unique()[0]\n",
    "    data.loc[val2_index,col] = data[col].unique()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa335e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14268104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CE_Individual_HoH_Has_Misc_Credit_Card.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca702f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THIS STEP\n",
    "data = data.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling remaining null values with mean/median/mode\n",
    "treatment_cols = [col for col in data.loc[:,data.isna().sum() > 0].columns]\n",
    "\n",
    "### Uncomment below section\n",
    "for col in treatment_cols:\n",
    "#     fill_value = data[f'{col}'].mean()\n",
    "    fill_value = data[f'{col}'].median()\n",
    "#     fill_value = data[f'{col}'].mode()\n",
    "    data.fillna(value=fill_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'7.Missing_val_treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51ca1d",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c61081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ff418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d51663",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = [x for x in data.columns if x not in binary_enc_cols]\n",
    "outlier_cols = [x for x in outlier_cols if x not in oneHot_enc_cols]\n",
    "outlier_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1398a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting indices for outliers in numeric columns\n",
    "outliers_indices_prob = []\n",
    "outliers_indices_poss = []\n",
    "\n",
    "# for col in [x for x in numeric_cols if x in data.columns and not in (binary_enc_cols and oneHot_enc_cols)]:\n",
    "for col in outlier_cols:\n",
    "    outliers_prob, outliers_poss = tukeys_method(data,col)\n",
    "    outliers_indices_prob.append(outliers_prob)\n",
    "    outliers_indices_poss.append(outliers_poss)\n",
    "    \n",
    "# dropping outliers indices\n",
    "drop_poss = 1   #0-> Probable Outliers | 1-> Possible Outliers\n",
    "\n",
    "if drop_poss:\n",
    "    combined_indices = itertools.chain.from_iterable(outliers_indices_poss)\n",
    "    drop_list = list(set(list(combined_indices)))\n",
    "else:\n",
    "    combined_indices = itertools.chain.from_iterable(outliers_indices_prob)\n",
    "    drop_list = list(set(list(combined_indices)))\n",
    "    \n",
    "data.drop(index=drop_list, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in outlier_cols if x not in freq_enc_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00154bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cea193",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if col in numeric_cols:\n",
    "        sns.boxplot(data=data, y=col, palette=sns.color_palette('muted'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7359e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if col in [x for x in numeric_cols if x in data.columns]:\n",
    "        sns.boxplot(data=data, y=col, palette=sns.color_palette('muted'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,data,'8.Outlier_records_drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1c1b3",
   "metadata": {},
   "source": [
    "# Pre-Clustering\n",
    "\n",
    "## Feature Scaling\n",
    "- Since data doesn't follow normal curve distribution Normalization is preferred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348951e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "print(f'Scaled data shape : {data_scaled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_data = pd.DataFrame(data_scaled, columns=data.columns)\n",
    "scaled_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,scaled_final_data,'9.Feature_scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14341f6e",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = scaled_final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ed6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with Variance(information) cut-off as 95%\n",
    "\n",
    "# pca = PCA(0.95)\n",
    "# principal_components = pca.fit_transform(scaled_final_data)\n",
    "# principal_df = pd.DataFrame(data = principal_components)\n",
    "# principal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d23966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with Variance(information) cut-off as 80%\n",
    "\n",
    "# pca2 = PCA(0.80)\n",
    "# principal_components2 = pca2.fit_transform(scaled_final_data)\n",
    "# principal_df2 = pd.DataFrame(data = principal_components2)\n",
    "# principal_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with Variance(information) cut-off as 70%\n",
    "\n",
    "pca3 = PCA(0.70)\n",
    "principal_components3 = pca3.fit_transform(scaled_final_data)\n",
    "principal_df3 = pd.DataFrame(data = principal_components3 , columns=list(f'Principal_component_{i + 1}' for i in range(len(principal_components3[0]))))\n",
    "principal_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components with respective variance accountibility with variance(information) cutoff as 95%\n",
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components with respective variance accountibility with variance(information) cutoff as 80%\n",
    "# pca2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components with respective variance accountibility with variance(information) cutoff as 70%\n",
    "pca3.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_export(data_pre,principal_df3,'10.PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b255f5d",
   "metadata": {},
   "source": [
    "### Dendrograms with 3 variance cut-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499877bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "# plt.title(\"Dendrograms\")\n",
    "# dend = shc.dendrogram(shc.linkage(principal_df, method='ward'),show_leaf_counts=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "# plt.title(\"Dendrograms\")\n",
    "# dend = shc.dendrogram(shc.linkage(principal_df2, method='ward'),show_leaf_counts=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "# plt.title(\"Dendrograms\")\n",
    "# dend = shc.dendrogram(shc.linkage(principal_df3, method='ward'),show_leaf_counts=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48618136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cluster_num:\n",
    "    cluster = AgglomerativeClustering(n_clusters=x, affinity='euclidean', linkage='ward')  \n",
    "    cluster_label3 = cluster.fit_predict(principal_df3)\n",
    "    print(silhouette_score(principal_df3, cluster_label3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc64c74",
   "metadata": {},
   "source": [
    "### Clustering with n=4 based on the above dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting cluster labels\n",
    "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')  \n",
    "# cluster_label = cluster.fit_predict(principal_df)\n",
    "\n",
    "# cluster_label2 = cluster.fit_predict(principal_df2)\n",
    "\n",
    "cluster_label3 = cluster.fit_predict(principal_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the delted records from base data\n",
    "base_data.drop(index=drop_list, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cluster label rows in original data\n",
    "\n",
    "# data_orig['Cluster'] = cluster_label\n",
    "# data_orig['Cluster2'] = cluster_label2\n",
    "base_data['Cluster'] = cluster_label3\n",
    "base_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa425f2",
   "metadata": {},
   "source": [
    "### Comparing Cluster label counts with different PCA outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_orig.Cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_orig.Cluster2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.Cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting the clustered data | Uncomment the below line\n",
    "\n",
    "base_data.to_csv(f\"{output_path}/Clustered_data.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
